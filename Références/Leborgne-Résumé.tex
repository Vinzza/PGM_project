\documentclass[a4paper,11pt,titlepage]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[english]{babel}
\usepackage{amsmath, amsfonts, amssymb, amsthm, dsfont}
\usepackage{bbold}
\usepackage{stmaryrd}
\usepackage{mathrsfs}
\usepackage{fancyhdr}
\usepackage{color}
\usepackage[colorlinks=true,linkcolor=blue]{hyperref}
\newtheorem{mydef}{Definition}

\begin{document}
\section{Results}
\subsection{Notations}
Cocktail party problem :
\begin{equation*}
	S \ \text{sources} \Rightarrow \mathcal{F}(X) \ \text{observations} \Rightarrow \mathcal{H}(y) \ \text{estimations}
\end{equation*}
Where $\mathcal{F}$ is the mixing function, and $\mathcal{H}$ is the separating function. $S = (s_1,\cdots,s_p)^T$ is the sources in $\mathds{R}^p$, $X$ is a random variable in $\mathds{R}^n$. We assume $p=n$.
\\
Linear case :
\begin{equation*}
	y = WX = WAS
\end{equation*}
We note $G = WA$
We consider the times series as matrices :
\begin{equation*}
X_T=\begin{pmatrix}
x_1(1)& \cdots & x_1(T) \\
\vdots & & \vdots \\
x_n(1)& \cdots & x_n(T) \\
\end{pmatrix}
\end{equation*}
We note the $i$-th rox of a matrix $W$ as $w_i$.

\subsection{PCA}
We note $V_x$ the covariance matrix of $x$, and
\begin{equation*}
	V_x = FDF^T
\end{equation*}
its diagonal reduction in an orthonormal basis.
\textbf{Result} If we denote $w_1$ the eigen vector corresponding to the greatest eigen value of $V_x$, then the data projection on $w_1$ is called the \textit{principal component}, and it is the one coding the most variance : $w_1 = \underset{|w|=1}{\text{argmax }}\mathds{E}[(w^Tx)^2]$.
We can compute $w$ using methods described in \textcolor{blue}{@article{oja1992principal,
title={Principal components, minor components, and linear neural networks},
author={Oja, Erkki},
journal={Neural Networks},
volume={5},
number={6},
pages={927--935},
year={1992},
publisher={Elsevier}
}
}
\\
We then have $W_{PCA}=D^{-1/2}F^T$. We call this operation \textit{spectral whitening} of the data. This basicly nullifies the data variances.

We can also have $W_{ZCA} = \mathds{E}[x^Tx]^{-1/2}$. Then the covariance matrix of $y = W_{ZCA}x$ is diagonal and our datas are uncorrelated. 
\section{ICA}
The major hypothesis of ICA is the statistical independance of our data. In the second hypothesis according to which the sources are linearly mixed, the first hypothesis is sufficient to provide a good source separation. \\
let $V_x$ be the covariance matrix of $x$. We factorize $V_x$ as:
\begin{equation*}
	V_x = AD^2A^T
\end{equation*}
Where $D \preccurlyeq 0$ is diagonal, and $A \in \mathcal{M}_{pn}$ is of rank $n$. We can put observations as $x = As$, where $D^2$ is the covariance matrix of $y$, and where $s$ is a vector of $\mathds{R}^n$ which is maximizing a certain \textit{contrast function} or \textit{independance measure} which can be found in \textcolor{blue}{@article{comon1994independent,
  title={Independent component analysis, a new concept?},
    author={Comon, Pierre},
  journal={Signal processing},
    volume={36},
  number={3},
    pages={287--314},
  year={1994},
    publisher={Elsevier}
}}
\\
Optimization algorithm found in \textcolor{blue}{@article{hyvarinen1999survey,
	  title={Survey on independent component analysis},
	    author={Hyvarinen, Aapo},
		  journal={Neural computing surveys},
		    volume={2},
			  number={4},
			    pages={94--128},
				  year={1999}
			  }}
For ICA we need the following conditions :
\begin{itemize}
	\item At most one source can follow a gaussian distribution
	\item the rank of $A$ must be the number of sources
\end{itemize}
First condition is due to the fact that a gaussian distribution as all this moment of order greater than 2 equal to 0. Thus the independance hypothesis is just a simple decorrelation such as the one done in PCA. The second condition can be lifted as shown in several papers (P51 du cours).
With dimensional reduction techniques, we can always assume the the number of observations is equal to the number of sources (which implies tha $A$ is squared). 
\\
Two last problems arise from ICA : it is insensible to a permutation of sources and a change in amplitude of a source. We can modelize this by a multiplication by a diagonal matrix of weight $D$ for the amplitude and a multiplication by the inverse of a permutation matrix $P$.

Let $p_y$ be the density of the estimated sources.
\paragraph{Contrast function} The contrast function $\Psi$ should be invariant to permutations : $\Psi(P.p_y) = \psi(p_y)$, scale invariant: $\Psi(p_{\Delta y})$ for any $\Delta$diagonal, and finaly discriminant for $y$ with mutualy independant composants : $\Psi(p_{My}) \geq \Psi(p_y)$ with equality only when $M$ is of the form $\Delta P$.
The mutual information is great for this problem, but hard to compute. Thus we use numerical approximation such as Gram-Charlier series.

\subsection{Neuronal approach}
The idea is to try to find $y$ as $y = (I+W)^{-1}x$, where the diagonal of $W$ is 0, and where $w_{ij} = f(y_i)g(y_j)$ elsewhere. $f$ and $g$ are supposed to be odd functions, several choices are given in the quoted articles (P53).
It is in the necessity to resort to statistics of higher order that lies the contribution of ICA.

\subsection{Learning approach}
\label{LA}
Using the contrast function, we define the ACI problem as an optimization one, and an iterative learning of the separation matrix. This kind of algorithms are dependant to the mixing matrix. Thus we use invariant estimator : $\hat{A}_{MX_T}=M\hat{A}_{X_T}$ ($\hat{A}_{X_T}$ is the estimation of $A$ obtain with $T$ samples $X$. We can see that with such an estimator of the sources, we have : $\hat{s}(t) = (\hat{A}_{S_T})^{-1}$.
We define the relative gradient :
\begin{equation*}
	W_{t+1} = (I - \lambda_t\nabla J_\psi(y_t))W_t
\end{equation*}
Where $\nabla J_\psi(y_t)$ is the gradient of a cost function depending on the contrast function $\psi$ computed on $y_t$. Then the serial update of the global source matrix $G =WA$ verifies :
\begin{equation*}
	G_{t+1} = (I - \lambda_t\nabla J_\psi(G_ts))G_t
\end{equation*}
\textbf{Results} For $\psi(y) = \sum\limits_{i=1}^n |y_i|^4$ and $J_\psi(y) = \mathds{E}[\psi(y)]$, we have $W_{t+1} = W_t -(y_t y_t^T-I+g(y_t)y_t^T- y_t g(y_t)^T)W_t$. \\
See \textcolor{blue}{@inproceedings{cardoso1996independent,
	  title={Independent component analysis, a survey of some algebraic methods},
	    author={Cardoso, Jean-Frcsn{\c{c}}ois and Comon, Pierre},
		  booktitle={Circuits and Systems, 1996. ISCAS'96., Connecting the World., 1996 IEEE International Symposium on},
		    volume={2},
			  pages={93--96},
			    year={1996},
				  organization={IEEE}
			  }
}
This idea is at the base of the HJ algorithm.
\subsection{Tensorial approach}
\label{TA}
The idea is to do tensorial diagonalization in order to optimize the contrast function, giving the \textit{JADE} algorithm, which follows the \textit{FOBI} one. The cumulant tensor of the $4$-th order is a 4D matrix containing all the 4-th order cumulant :$N =(Cum(x_i,x_j,x_k,x_l))_{i,j,k,l}$. We can also view $N$ as a linear application of $\mathcal{M}_{nn}$ to $\mathcal{M}_{nn}$, and see $N$ as a matrix of size $n^2 \times n^2$.
The aim of this algorithm is then to diagonalize $N$ which, as $N$ is normalized, is equivalent to try to maximize the diagonal elements of $N$. It can be shown that the contrast function is then $c(x) =\sum\limits_{i,k,l} |Cum(x_i,x_i*,x_k,x_l*)|^2$.\\
This algorithm is of the monstruous complexity of $\mathcal{O}(n^4)$, and thus unusable for real data\dots

\subsection{Maximum likelihood}
Likelihood of the observations given our mixing matrix :
\begin{equation*}
	p_{x|A}(y) = \int p_s(A^{-1}u)|\det(A)|^{-1}du
\end{equation*}
If we note $\Phi_i = [\log(p_{s_i})]'$, then the maximum likelihood estimator is obtained by solving :
\begin{equation*}
	\hat{\mathds{E}}[\Phi_i(e_i^TA^{-1}x)e_j^TA^{-1}x] = 0
\end{equation*}
With $\hat{s}_i= e_i^TA^{-1}x$ the estimation of our sources, we get the simple condition :
\begin{equation*}
	\hat{\mathds{E}}[\Phi_i(\hat{s}_i)\hat{s}_j] = 0
\end{equation*}
This becomes an optimization problem, we can use what we want to solve it\dots
This approach as been proved to be equivalent to the \textit{Infomax} method by Cardoso.


\subsection{ICA as non linear PCA}
We can derive an andaptation rule for neurons network learning :
\begin{equation*}
	W_{t+1} = W_t +\lambda_t [x_t g_1(e_t^T)W_t G_2(x_t^TW_t)+ g_1(e_t)f_2(x_t^TW_t)]
\end{equation*}
Where $f_1,f_2$ are two non-linear functions, $g_1 = f_1',g_2=f_2'$ , $G_2 = \text{diag}(g_2(.))$ and $e_t = x_t-W_tg_2(W_t^Tx_t)$
For stability reasons, $g_1$ needs to be a impaire croissante function. With $f_1$ quadratic and $f_2$ lineal, this is the standard PCA.\\
With $W$ orthogonal and $y=Wx$, we have :
\begin{equation*}
	\| x-W^Tg(Wx) \|^2 = \sum\limits_{i=1}^n [y_i-g(y_i)]^2
\end{equation*}
\subsection{Infomax}
Theorticaly equivalent to the learning approach. From neurons network theory :
\begin{equation*}
	\frac{\partial I}{\partial w}(x,y) = \frac{\partial H}{\partial w}(y)
\end{equation*}
Where I(x,y) is the mutual information between inputs $x$ and outputs $y$ of the neurones network, $H(y)$ is the ouput entropy and $w$ the network parameters.
Thus the update rule :
\begin{equation*}
	\Delta W = (W^{-T}) + \frac{\partial}{\partial w} \ln \prod\limits_i |y'_i|
\end{equation*}

This rule can be upgraded for faster convergence and better results into:
\begin{equation*}
	\Delta W = [I -K \tanh(y)y^T-yy^T]W
\end{equation*}
Where K is a diagonal matrix with 1 for an over gaussian source, and -1 for a sub gaussian one.
\subsection{Non gaussianity measure}
Accordiang to CLT, a sum of independant variables converges toward a gaussiand distribution. Thus we try to have estimations as much non-gaussian as possible. A robust measure of non-gaussianity is the negentropy : $J(y) = H(y_{gauss})-H(y)$ where H is the classical entropy, and $y_{gauss}$ is a gaussian variable of same mean and variance as y. To obtain even more robust estimators, we approach negentropy by : $J(y) \propto [\mathds{E}[G(y)]-\mathds{E}[G(\mu)]^2$.
Where G is a non quadratic function. The algorithm (which is based on the last section update rule) needs uncorrelated and centered data at each iteration, thus an orthogonalization step.\\

The orthonormalization step can be done either column by column (Deflation) or all matrix directly (symmetric).

\section{Algorithms}
\subsection{HJ}
Described in section \ref{LA}. One matlab version found.
\subsection{CoM1-2}
Using contrast functions and cumulants. Introuvable\dots
\subsection{Jade}
Described in section \ref{TA}. One matlab version (Cardoso's ma gueule) found.
\subsection{Fast-ICA}
Described the last four sections. Available in matlab and python packages.
\subsection{BS}
Based on infomax and neurones networks. Extract matlab from \\\url{ftp://ftp.cnl.salk.edu/pub/tony/sep96.public} .
\section{Applications}
	\begin{itemize}
		\item Speech signals
		\item Medical imagery : EEG and MEG, \bf{MRI}
		\item Finance : look for independant factors explaining the actions market structure
		\item Analyze of the money flux of 40 stores of a same chain over 3 years to see temporal events or relation to other chains.
		\item Temporal series (Stephaaaaane Mallaaaaaaaat) construct a predictor
		\item Use on natural images to mimique the brain 
		\item Image recognition and classification by extraction of patterns
		\item Multimedia data modelization
		\item Audio-Video fusion
		\item Image compression *
		\item Image denoizing *
		\item Transparence separation (photo through galss) *
	\end{itemize}
* Not studied enough, or not an improvement enough at the time.
\end{document}


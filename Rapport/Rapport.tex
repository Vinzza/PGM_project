\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}

\input{zMC.tex}
\input{zMCgraphe.tex}

\zzpackages[english]
\zzhyperref
\zzmarges

\zzheader{PGM Project}{}{\today}{De Lara, Tilquin, Vidal}{}{\arabic{page}/\pageref{lastpage}}

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                MACRO LOCALES                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %%

% Divergence de Kullback-Leibler
\newcommand{\Kl}[3][]{\mathrm K_{#1}\!\zp{#2\:\|\:#3}}
\newcommand{\zZ}[2]{\mathrm #1\!\zp{#2}}
\newcommand{\zD}{\mathcal}
\newcommand{\Ng}[2]{\mathcal{N}\zp{#1,\:#2}}

\tikzset{
  zplot/.style={opacity=.8}
}

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                DEBUT DOCUMENT                                %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %%

\begin{document} 

\def\foo#1#2{\vbox{\hbox to 2cm{\hss\scshape #1\hss}\hbox to 2cm{\hss\footnotesize#2\hss}}\hss}

\begingroup\centering
{\bfseries \huge Independent Component Analysis}\par\vspace{.3cm}
{\bfseries \Large PGM Project}\par\vspace{.7cm}
\hbox to \textwidth{\hss
\foo{Nathan de Lara}{École polytechnique}
\foo{Florian Tilquin}{ENS Cachan}
\foo{Vincent Vidal}{ENS Ulm}
}\par\vspace{.8cm}
\today\zal\vspace{.3cm}\zal
\zligne\endgroup

\tableofcontents

\vspace{1cm}\zligne


\begin{center}\textsc{Abstract}\end{center}
This paper is dedicated to the study of Independent Component Analysis. We intent to implement, apply and compare several algorithms while being presenting some theoretical aspects such as the link between the likelihood maximisation and the mutual information.

%% ------- -- -- -  -                                      -  - -- -- --------%%
%---- -- -  -                     Introduction                     -  - -- ----%
%%-------- -- -- -  -                                      -  - -- -- ------- %%


\section{Problem statement}
\subsection{Introduction}

The general Independent Component Analysis problem can be formalised this way:
Suppose we have some random variables $x\in\zR^p$ which correspond to
a mix of some primitive sources $s\in\zR^n$. The aim is to extract
from $x$ every source $s_i$. To do so, we will suppose here that:
\zitemize{
\item[--] the sources are independents.
\item[--] the mix is linear and instantaneous
\item[--] at most one source has a Gaussian distribution.
}
We define:
\begin{equation}
  x = A s \ \ \mbox{and} \ \ y = W x,
\end{equation}
where $A$ is the mixing matrix, $W$ the separation matrix and $y$ the
estimation of the sources. The goal is then to find a matrix $W$ that
maximise a certain measure of independence of $y$.

As a measure of independence, we consider, for theoretical purpose,
the mutual information:\begin{equation}
  \zZ IY = \int_{\zR^p} P(Y) \log\frac{P(Y)}{\Pi_i P_i(Y_i)} \zdx Y.
\end{equation}
However, as it is too hard to compute, we consider other contrast functions, invariant by permutation, scaling on coordinates and maximal for independent ones.


%% --------- ---- -  -         Information Theory         -  - ---- --------- %%

\subsection{Information Theory}
\label{infth}
Let $X\in\zR^n$ be a random variable, we note $P\zp{X}$ his density and $\Sigma_X$ his covariance matrix.
\znl

In the space of measures, let $\zD G$ be the manifold of Gaussian distributions, $\zD P$ the manifold of ``product'' distributions and $\zD P\wedge\zD G$ the manifold of Gaussian ``product'' distributions. Note that these manifolds are exponential families.

The main advantage of this geometric point of view is that the Kullback-Leibler divergence allows the notion of projection on exponential families. The projection of $P$ on the family $\zD E$ is defined as the vector of $\zD E$ that minimise the divergence to $P$. We write this projection~$P^{\zD E}$.
\znl

Then, we define:

The \textbf{Kullback--Leibler divergence} distribution from $Q$ to $P$:
\begin{equation}
  \Kl PQ = \int_{\zR^n} P(x) \log\frac{P(x)}{Q(x)} \zdx x.
\end{equation}

The \textbf{entropy}:\begin{equation}
  \zZ HP = - \int_{\zR^n} P(x)\log P(x) \zdx x.
\end{equation}

The \textbf{mutual information}:
\begin{equation}
  \zZ IY = \Kl {\zpbig2 P(Y)}{\Pi_i P_i(Y_i)} = \Kl {\zpbig2 P(Y)}{P(Y)^{\zD P}}.
\end{equation}

The \textbf{negentropy}:
\begin{equation}
  \mathrm{G}_n(Y) = \zZ H{\Ng{\zesp Y}{\Sigma_Y}\zpbig2}-\zZ HY= \zZ H{P(Y)^{\zD G}}-\zZ H{P(Y)}.
\end{equation}


The \textbf{non-gaussianity}:
\begin{equation}
  \zZ GY = \Kl{Y}{\Ng{\zesp Y}{\Sigma_Y}\zpbig2} = \Kl {\zpbig2 P(Y)}{P(Y)^{\zD G}}.
\end{equation}

The \textbf{correlation}:
\begin{equation}\begin{array}{rcl}
  \zZ CY &=& \displaystyle\Kl{\Ng{\zesp Y}{\Sigma_Y}\zpbig2}{\Ng{\zesp Y}{\mathrm{Diag}\:\Sigma_Y}}\\
  &=&\Kl{\zpbig2 P(Y)^{\zD G}}{P(Y)^{\zD P\wedge\zD G}}\\
  &=& \displaystyle\frac 12\log\frac{\det\zp{\mathrm{Diag}(\Sigma_Y)}}{\det\zp{\Sigma_Y}}.
\end{array}\end{equation}
\begin{figure}\centering
\begin{tikzpicture}[xscale=5, yscale=4, zvar/.style={line width = 1}, zdis/.style={scale=1}, zindic/.style={scale=.8}, zindict/.style={sloped, scale=.8}]
  \draw[zvar](0,-.3) .. controls (.1,0) and (0,.8) .. node[zindict, anchor=north]{Correlation} node[zindic, anchor=west]{$\zZ CP$} (-.1,1) node[anchor=0, scale=1.4]{$\zD G$};
  \draw[zvar](-.3,0) .. controls (0,.1) and (.7,0) .. node[pos=.54, zindict, anchor=north]{Marginal Gaussianity} node[pos=.54, zindic, anchor=south]{$\sum_i\zZ G{P_i}$} (1,-.1) node[anchor=130, scale=1.4]{$\zD P$};
  \draw (.8,.9) node[anchor=south west,zdis]{$P$} .. controls (.6,.9) and (.1,.75) .. node[zindict, anchor=-70]{Gaussianity} node[zindic, anchor=110]{$\zZ GP$} (-.02,.7) node[anchor=east,zdis]{$P^{\zD G}$};
  \draw (-.01,.65) -- (.04,.66) -- (.03,.72);
  \draw (.8,.9) .. controls (.85,.7) and (.9,.2) .. node[zindict, anchor=south]{Dependence} node[zindic, anchor=east]{$\zZ IP$} (.8,-.05) node[anchor=north,zdis]{$P^{\zD P}$};
  \draw (.75,-.035) -- (.765,.025) -- (.82,.01);
  \draw (.8,.9) .. controls (.7,.6) and (.05,.04) .. node[zindic,rotate=38,scale=.8, anchor=south,pos=.45] {$\Kl P{P^{\zD P\wedge\zD G}}$} (.037,.037);
\node[anchor=north east,zdis] at (.05,.04) {$P^{\zD G\wedge \zD P}$};
\foreach \x/\y in{.8/.9,.04/.04,.8/-.045,-.02/.7}{\draw[fill,shift={(\x,\y)}, xscale=.80] (0,0) circle (.013);}
\end{tikzpicture}  
\caption{Representation of a distribution $P$ and the different projections on the exponential families $\zD P$ and $\zD G$. On the paths between the distributions are the quantities associated to the Kullback-Leibler divergence between those distributions.}\label{fig:pythagorean}
\end{figure}

Using the Pythagorean theorem and the two decompositions of $\Kl P{P^{\zD P\wedge\zD G}}$, through $P^{\zD P}$ or $P^{\zD G}$, shown in the Figure \ref{fig:pythagorean}, we can prove that:
\begin{equation}
        \zZ IY + \sum_i\zZ G{Y_i} = \zZ GY + \zZ CY.
\end{equation}
For more information see~\cite{cardoso2003}.

\subsection{ICA and Maximum Likelihood}
As presented in~\cite{hyvarinen2000}, it is possible to consider ICA as a maximum likelihood problem linked to the infomax principle. 
With the previously introduced notations, the log-likelihood is defined as:
\begin{equation}
L = \sum_t \sum_i \log f_i(w_i^Tx(t))+T\log(|det(W)|)
\end{equation}
Where $f_i$ is the density function of $s_i$. The expectation of this likelihood is:
\begin{equation}
\mathbb{E}[L] = \sum_i \mathbb{E}[\log f_i(w_i^Tx(t))]+\log(|det(W)|)
\end{equation}
In the case where $f_i$ is the actual distribution of $w_i^Tx(t)$, the first term becomes $-\sum_i H(w_i^Tx(t))$ which is one of the independence measures listed in~\ref{infth}.

%% ------- -- -- -  -                                      -  - -- -- --------%%
%---- -- -  -                     Algorithmes                      -  - -- ----%
%%-------- -- -- -  -                                      -  - -- -- ------- %%

\section{Algorithms for ICA}
\subsection{Hérault and Jutten (HJ) algorithm}
This method is based on the neural network principle.
We write $W = \zp{I_n+\widetilde W}^{-1}$ and for a pair of given functions $\zp{f,\:g}$, we adapt $\widetilde W$ as follows:\begin{equation}
\widetilde W_{ij} = f(y_i) g(y_j).
\end{equation}

%% --------- ---- -  -            -----------             -  - ---- --------- %%

\subsection{Jade algorithm}
Several methods are based on the cumulants. The goal here is to annul all the cross cumulants of order $4$.
Thus, the idea is to diagonalize the cumulant tensor which is equivalent to minimise the following contrast function:
\begin{equation}
  c\zp{x} = \sum_{i,k,l}\zp[b]{\zop{Cum}{x_i,x_i^*,x_k,x_l}}^2.
\end{equation}

%% --------- ---- -  -            -----------             -  - ---- --------- %%

\subsection{FastICA algorithm}
The FastICA algorithm is based on the information theory. The goal here is to maximise the marginal non-gaussianity on the whitened data, relying on a non linear quadratic function $f$ with the following rule:\begin{equation}
  \widetilde W_{t+1} = \zesp{X.\ztr{f(\ztr{W_t}X)}} - \zesp{f''(\ztr{W_t}X)}W_t,
\end{equation}
with $W_t$ the normalise vector of $\widetilde W_t$. In our experiments, we used $f(x) = \frac{x^4}4$. But it is possible to use $f(x) = \log \cosh x$ or $f(x) = \exp\zp{-\frac{x^2}2}$ as well.

\subsection{Kernel ICA algorithm}
Given a reproducing kernel Hilbert space $\mathcal{F}$, this algorithm seeks to minimize the Kernel Generalized Variance defined as:
\begin{equation}
	\widehat{\delta}_{\mathcal{F}}=-\frac{1}{2}\log \underset{i}{\prod}(1-\rho_i^2)
\end{equation}
where the $\rho_i$ are the kernel canonical correlations between the observations components, obtained with computations over the observations Gram matrices.

%% ------- -- -- -  -                                      -  - -- -- --------%%
%---- -- -  -                      Resultats                       -  - -- ----%
%%-------- -- -- -  -                                      -  - -- -- ------- %%

\section{Results}
\paragraph{Performance measure}
The ``Amari divergence'', equation \ref{eqn:amari}, gives a criterion of proximity between two matrices, to evaluate the performance of an algorithm.
If $U$ and $V$ are two $n$-by-$n$ matrices, the Amari error is defined by:
\begin{equation} \label{eqn:amari}
	d(U,V) = \frac{1}{2n}\sum\limits_{i=1}^n \left(\frac{\sum\limits_{j=1}^n|a_{ij}|}{\max_j |a_{ij}|}-1 \right)+\frac{1}{2n}\sum\limits_{j=1}^n \left(\frac{\sum\limits_{i=1}^n|a_{ij}|}{\max_i |a_{ij}|}-1 \right)
\end{equation}
with $a_{ij} = (UV^{-1})_{ij}$.
This function, which is not an actual distance, has the advantage to be invariant by scaling factors and permutations of the matrices components.

\begin{figure}
\label{distrib}
\centering
\includegraphics[scale=0.9]{graph_distrib.pdf}\\
\caption{Distributions used to test the algorithms.}
\end{figure}

\begin{figure}
\label{distres}
\centering
%\resizebox{\textwidth}{!}{
\input{Tableau_Nathan.txt}
\vspace{3mm}
\input{Tableau2.txt}%}
\caption{\textbf{Left:} Average Amari divergence re-scaled by 100 obtained with the listed algorithms for random mix $m=2$ sources of size $N=250$ sampled with twelve different distributions. \textbf{Right:} Same measure for $m$ sources of size $N$ whose distributions are randomly selected among the twelve. The best results are in bold font. An X is put when a standard desktop computer could not compute the result.}
\end{figure}

\begin{figure}
\label{imres}
\centering
\includegraphics[width=.74\textwidth]{../image_test/unmix4.png}
\caption{Application of JADE algorithm to images separation. The first line presents the original sources, the second one the mix and the last one the estimations.}
\end{figure}

%% \begin{figure}\begin{center}
%% \hspace{-.8cm}\begin{mygraph}{xmin=0, xmax=100, %
%%                 ymin=-1, ymax=1, %
%%                 sizex=5, sizey=2.5}%
%%                 {nomx=Sources, nomy=}%
%%                 {0,50,100}{-1,-.5,...,1.05}
%%   \draw[zplot, red] plot file {data/init_sources_2.txt};
%%   \draw[zplot, green!70!black] plot file {data/init_sources_3.txt};
%%   \draw[zplot, blue] plot file {data/init_sources_1.txt};
%% \end{mygraph}\hspace{-1cm}
%% \begin{mygraph}{xmin=0, xmax=100, %
%%                 ymin=-1, ymax=1, %
%%                 sizex=5, sizey=2.5}%
%%                 {nomx=FastICA, nomy=}%
%%                 {0,50,100}{-1,-.5,...,1.05}
%%   \draw[zplot, red] plot file {data/init_fast_1.txt};
%%   \draw[zplot, green!70!black] plot file {data/init_fast_2.txt};
%%   \draw[zplot, blue] plot file {data/init_fast_3.txt};
%% \end{mygraph}

%% \begin{mygraph}{xmin=0, xmax=100, %
%%                 ymin=-1, ymax=1, %
%%                 sizex=5, sizey=2.5}%
%%                 {nomx=HJ, nomy=}%
%%                 {0,50,100}{-1,-.5,...,1.05}
%%   \draw[zplot, red] plot file {data/init_hj_2.txt};
%%   \draw[zplot, green!70!black] plot file {data/init_hj_1.txt};
%%   \draw[zplot, blue] plot file {data/init_hj_3.txt};
%% \end{mygraph}\hspace{-1cm}
%% \begin{mygraph}{xmin=0, xmax=100, %
%%                 ymin=-1, ymax=1, %
%%                 sizex=5, sizey=2.5}%
%%                 {nomx=JADE, nomy=}%
%%                 {0,50,100}{-1,-.5,...,1.05}
%%   \draw[zplot, red] plot file {data/init_jade_3.txt};
%%   \draw[zplot, green!70!black] plot file {data/init_jade_1.txt};
%%   \draw[zplot, blue] plot file {data/init_jade_2.txt};
%% \end{mygraph}
%% \vspace{-.2cm}
%% \caption{Results of the main ICA algorithms on simulated data. Each colour corresponds to a signal. The ``Sources'' graph shows the unmixed signals and the other ones the results of the algorithms. \label{fig:res}}
%% \end{center}\end{figure}


%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                               FIN DU DOCUMENT                                %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %%


\bibliographystyle{alpha}
\bibliography{Biblio}{}
\nocite{*}

\label{lastpage}

\end{document}

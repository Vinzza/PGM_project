\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}

\input{zMC.tex}
\input{zMCgraphe.tex}

\zzpackages[english]
\zzhyperref
\zzmarges

\zzheader{PGM Project}{}{\today}{De Lara, Tilquin, Vidal}{}{\arabic{page}/\pageref{lastpage}}

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                MACRO LOCALES                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %%

% Divergence de Kullback-Leibler
\newcommand{\Kl}[3][]{\mathrm K_{#1}\!\zp{#2\:\|\:#3}}
\newcommand{\zZ}[2]{\mathrm #1\!\zp{#2}}
\newcommand{\zD}{\mathcal}
\newcommand{\Ng}[2]{\mathcal{N}\zp{#1,\:#2}}

% Mettre en valeur un résultat (pour pouvoir changer de style sans avoir à se taper l'intégralité du tex)
\newcommand{\zmev}[1]{\textbf{#1}}

\tikzset{
  zplot/.style={opacity=.8}
}

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                DEBUT DOCUMENT                                %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %%

\begin{document} 

\def\foo#1#2{\vbox{\hbox to 2cm{\hss\scshape #1\hss}\hbox to 2cm{\hss\footnotesize#2\hss}}\hss}

\begingroup\centering
{\bfseries \huge Independent Component Analysis}\par\vspace{.3cm}
{\bfseries \Large PGM Project}\par\vspace{.7cm}
\hbox to \textwidth{\hss
\foo{Nathan de Lara}{École polytechnique}
\foo{Florian Tilquin}{ENS Cachan}
\foo{Vincent Vidal}{ENS Ulm}
}\par\vspace{.8cm}
\today\zal\vspace{.3cm}\zal
\zligne\endgroup

%% ------- -- -- -  -                                      -  - -- -- --------%%
%---- -- -  -                     Introduction                     -  - -- ----%
%%-------- -- -- -  -                                      -  - -- -- ------- %%


\section{Problem statement}
\subsection{Introduction}

Reintroducing the notations used in~\cite{LeBorgne}, the general Independent Component Analysis problem can be formalised this way:
Suppose we have some random variables $x\in\zR^p$ which correspond to
a mix of some primitive sources $s\in\zR^n$. The aim is to extract
from $x$ every source $s_i$. It appears that $s$ can only be recovered up to a scaling factor and a permutation of the components. To do so, we suppose here that:
\zitemize{
\item[--] the sources are independents.
\item[--] the mix is linear and instantaneous
\item[--] at most one source has a Gaussian distribution.
}
We formally define:
\begin{equation}
  x = A s \ \ \mbox{and} \ \ y = W x,
\end{equation}
where $A$ is the mixing matrix, $W$ the separation matrix and $y$ the
estimation of the sources. The goal is then to find a matrix $W$ that
maximises a certain measure of independence of $y$.

As a measure of independence, we consider, for theoretical purpose,
the mutual information, defined in equation \ref{eqn:def_I}.
However, as it is too hard to compute, we consider other contrast functions, invariant by permutation, scaling on coordinates and maximal for independent ones.


%% --------- ---- -  -         Information Theory         -  - ---- --------- %%

\subsection{Information Theory}
\label{infth}
Let $X\in\zR^n$ be a random variable, we note $P(X)$ its density and $\Sigma_X$ its covariance matrix. In the following, we write only $P$ when it is defined. Besides we note $\zZ HX$ the entropy of $X$, defined as $\zesp{-\log P(X)}$.

\znl

In the space of measures, let $\zD G$ be the manifold of Gaussian distributions, $\zD P$ the manifold of ``product'' distributions and $\zD P\wedge\zD G$ the manifold of Gaussian ``product'' distributions. Note that these manifolds are exponential families.
In this space, we can define the \zmev{Kullback--Leibler divergence} from $Q$ to $P$ :
\begin{equation}
  \Kl PQ = \int_{\zR^n} P(x) \log\frac{P(x)}{Q(x)} \zdx x.
\end{equation}
The main advantage of this geometric point of view is that the Kullback-Leibler divergence allows the notion of projection on exponential families.
The projection of $P$ on the family $\zD E$, noted $P^{\zD E}$, is defined as the vector of $\zD E$ that minimise the divergence to $P$.
This projection verifies the Pythagorean theorem and implies relations between the main quantities defined with this divergence.
Schematized in the figure \ref{fig:pythagorean}, the main quantities are:

-- The \zmev{mutual information}:
\begin{equation}\label{eqn:def_I}\begin{array}{rcl}
  \zZ IY &=&\displaystyle \Kl {\zpbig2 P(Y)}{\Pi_i P_i(Y_i)} \quad=\quad \Kl {\zpbig2 P(Y)}{P(Y)^{\zD P}}\\
&=&\displaystyle \sum_i \zZ H{\zpbig1P(Y_i)} - \zZ H{\zpbig1P(Y)}.
\end{array}\end{equation}

-- The \zmev{non-gaussianity}:
\begin{equation}\begin{array}{rcl}
  \zZ GY &=& \Kl{Y}{\Ng{\zesp Y}{\Sigma_Y}\zpbig2} \quad=\quad \Kl {\zpbig2 P(Y)}{P(Y)^{\zD G}}.
\end{array}\end{equation}

-- The \zmev{correlation}:
\begin{equation}\begin{array}{rcl}
  \zZ CY &=& \displaystyle\Kl{\Ng{\zesp Y}{\Sigma_Y}\zpbig2}{\Ng{\zesp Y}{\mathrm{Diag}\:\Sigma_Y}}\\
  &=&\Kl{\zpbig2 P(Y)^{\zD G}}{P(Y)^{\zD P\wedge\zD G}}\\
  &=& \displaystyle\frac 12\log\frac{\det\zp{\mathrm{Diag}(\Sigma_Y)}}{\det\zp{\Sigma_Y}}.
\end{array}\end{equation}

\begin{figure}\centering
\includegraphics{figure_tikz/theory_info.pdf}
\caption{Representation of a distribution $P$ and the different projections on the exponential families $\zD P$ and $\zD G$. On the paths between the distributions are the quantities associated to the Kullback-Leibler divergence between those distributions.}\label{fig:pythagorean}
\end{figure}

Using the Pythagorean theorem and the two decompositions of $\Kl P{P^{\zD P\wedge\zD G}}$, through $P^{\zD P}$ or $P^{\zD G}$, shown in the Figure \ref{fig:pythagorean}, we can prove that:
\begin{equation}
        \zZ IY + \sum_i\zZ G{Y_i} = \zZ GY + \zZ CY.
\end{equation}
Because the non-gaussianity is invariant under invertible affine transforms, minimising the mutual independence according to $W$ is equivalent to minimise $\zZ CY - \sum_i\zZ G{Y_i}$. We can then define a set of contrast function, for $\alpha\geq 0$:
\begin{equation}
  \phi_\alpha(Y) = \alpha \zZ CY - \sum_i\zZ G{Y_i}.
\end{equation}
Let's remark that the FastICA algorithm is based on the minimisation of the marginal non-gaussianity (so $\alpha = 0$). For more information see~\cite{cardoso2003}.

%% --------- ---- -  -     ICA and Maximum Likelihood     -  - ---- --------- %%

\subsection{ICA and Maximum Likelihood}
As presented in~\cite{hyvarinen2000}, it is possible to consider ICA as a maximum likelihood problem linked to the infomax principle. 
With the previously introduced notations, the log-likelihood is defined as:
\begin{equation}
L = \sum_{t=1}^{T} \sum_i \log f_i\zp{\ztr{w_i}x(t)}+T.\log\zp{|det(W)|},
\end{equation}
where $f_i$ is the density function of $s_i$. Then, if we suppose $f_i$ be the actual distribution of $y_i(t) = \ztr{w_i}x(t)$, the expectation of this likelihood can be written :
\begin{equation}\begin{array}{rcl}
\zesp{L} &=& \displaystyle\log\zp[b]{\zpbig1\mathrm{det}W}+\sum_i \zesp{\log f_i\zp{\ztr{w_i}x(t)}}\\
&=& \zZ H{WX} - \zZ HX - \displaystyle\sum_i\zesp{-\log P\zp{y_i(t)}},
\end{array}\end{equation}
which is, up to a constant $\zZ HX$, the Mutual Independence given equation \ref{eqn:def_I}.

\subsection{Performance measure}

Because we can only recover the matrix $W$ up to scaling factors and a permutation of the component, we can't norms to evaluate the separation matrix.
We use here the ``Amari divergence'' \cite{amari1996new}, written in equation \ref{eqn:amari}, which gives a criterion of proximity between two matrices.

If $U$ and $V$ are two $n$-by-$n$ matrices, the Amari error is defined by:
\begin{equation} \label{eqn:amari}
  d(U,V) = 
  \frac 1{2n} \sum_i \zp{\frac{\sum_j|a_{ij}|}{\underset j\max\: |a_{ij}|}-1}
+\frac 1{2n}\sum_j \zp{\frac{\sum_i|a_{ij}|}{\underset i\max\: |a_{ij}|}-1},
\end{equation}
with $a_{ij} = (UV^{-1})_{ij}$.
This function, which is not an actual distance, has the advantage to have the invariant wanted: invariant by scaling factors and permutations matrix multiplication.


%% ------- -- -- -  -                                      -  - -- -- --------%%
%---- -- -  -                     Algorithmes                      -  - -- ----%
%%-------- -- -- -  -                                      -  - -- -- ------- %%

\section{Algorithms for ICA}
\subsection{Hérault and Jutten (HJ) algorithm}
This method is one of the first algorithm to perform ICA, see \cite{jutten1991blind}. It is based on the neural network principle.
We write $W = \zp{I_n+\widetilde W}^{-1}$ and for a pair of given functions $\zp{f,\:g}$, we adapt $\widetilde W$ as follows:\begin{equation}
\widetilde W_{ij} = f(y_i) g(y_j).
\end{equation}

%% --------- ---- -  -            -----------             -  - ---- --------- %%

\subsection{Jade algorithm}
JADE algorithm, see \cite{cardoso1989source} is the most famous algorithm based on the cumulants. The goal here is to annul all the cross cumulants of order $4$, which would assure a certain ``independence'' up to the order $4$.
Thus, the idea is to diagonalize the cumulant tensor which is equivalent to minimise the following contrast function:
\begin{equation}
  c\zp{x} = \sum_{i,k,l}\zp[b]{\zop{Cum}{x_i,x_i^*,x_k,x_l}}^2.
\end{equation}

%% --------- ---- -  -            -----------             -  - ---- --------- %%

\subsection{FastICA algorithm}
This algorithm introduced by Hyvarinen~\cite{hyvarinen99} is based on the maximisation of the marginal non-gaussianity, which is approximated here as follow:
\begin{equation}\label{eqn:fast_prob}
  \zZ G{Y_i} \simeq C^{te}\times\zp{\zesp{f(Y_i)} - \zesp{f(\Ng 01)}}^2,
\end{equation}
with $C^{te}$ a positive constant and $f$ a non linear quadratic function.
In our experiments, we used $f(x) = \frac{x^4}4$. But it is possible to use $f(x) = \log \cosh x$ or $f(x) = \exp\zp{-\frac{x^2}2}$ as well.
The problem is now an optimisation problem consisting of maximising the expression in equation \ref{eqn:fast_prob} according to $W$ under the constraint of non correlation of $y = Wx$. Using a Fixed-point algorithm, we could derive the following iteration step:
\begin{equation}
  \widetilde W_{t+1} = \zesp{X.\ztr{f(\ztr{W_t}X)}} - \zesp{f''(\ztr{W_t}X)}W_t,
\end{equation}
with $W_t$ the normalise vector of $\widetilde W_t$.

\subsection{Kernel ICA algorithm}
Given a reproducing kernel Hilbert space $\mathcal{F}$, this algorithm seeks to minimize the Kernel Generalized Variance defined as:
\begin{equation}
	\widehat{\delta}_{\mathcal{F}}=-\frac{1}{2}\log \underset{i}{\prod}(1-\rho_i^2),
\end{equation}
where the $\rho_i$ are the kernel canonical correlations between the observations components, obtained with computations over the observations Gram matrices.

%% ------- -- -- -  -                                      -  - -- -- --------%%
%---- -- -  -                      Resultats                       -  - -- ----%
%%-------- -- -- -  -                                      -  - -- -- ------- %%

\section{Results}

\subsection{Experimental design for simulated data}

\begin{figure}
\centering
\includegraphics[scale=0.9]{figure_tikz/graph_distrib.pdf}\\
\caption{Distributions used to test the algorithms.\label{fig:distrib}}
\end{figure}

In order to test the different algorithms, we use a pipeline from \cite{bach2003kernel}.
We select some distributions, whose density are shown figure \ref{fig:distrib},
sample $N$ times a certain amount $m$ of them to create the initial signal $s$. Note that we may choose a given distribution multiple times.
Then, we sample a random bounded matrix $A$, used to mix the signals and perform withening.

At this point, we apply the ICA algorithms to the signal $Y = PAs$, which gave us the separation matrix $W$.
Eventually we evaluate the performance of the algorithm by computing the ``Amari divergence'' between $W$ and the real separation matrix $W_0 = \zp{PA}^{-1}$.

Note that the whitening matrix $P$ corresponds to the inverse of the square root of the covariance matrix of $X=As$.

\subsection{Results on experimental data}

The results are showed in the table \ref{tab:resultats}.

First, two sources following the same distribution are mixed.
The proximity of the distribution $8$, $9$ and $12$ to the gaussian, prohibited
for more than one source, explains the overall bad results obtained for this
distributions.

Then, the number of distribution $m$ and the number of samples $N$ vary and distributions are randomly selected (one distribution is
potentially selected multiple times). We can see here the influence of
computational cost. The Kernel ICA and the HJ algorithms couldn't manage to
achieve results as the number of sources grew. Let us note that the poor results of the HJ algorithm on $4$ sources can be explained by the non convergence of this algorithm.


Overall, kernel ICA and HJ algorithms perform better than the two others, but
require a higher computation time.

\begin{table}
\centering
\hbox to \textwidth{\hspace{-.5cm}
\resizebox{.45\textwidth}{2cm}{\input{Tableau_Nathan.txt}}\hss
\resizebox{.55\textwidth}{2cm}{\input{Tableau2.txt}}
}
\caption{\textbf{Left:} Average Amari divergence re-scaled by 100 obtained with the listed algorithms for random mix $m=2$ sources of size $N=250$ sampled with twelve different distributions. \textbf{Right:} Same measure for $m$ sources of size $N$ whose distributions are randomly selected among the twelve. The best results are in bold font. An X is put when a standard desktop computer could not compute the result.\label{tab:resultats}}
\end{table}

\subsection{Results on real data}

We took $4$ images and mixed them with a random bounded matrix.
The $4$ mixed images were given to the ICA algorithm and the $4$ separate images were normalise and display.
All the images for the JADE algorithm are showed in the figure \ref{fig:res_images}.

The results are quite good, even if we can still see some artefacts of the other images. Let's remark that, as expected, the recovered images are permuted compared to the initial ones and that $2$ of them are in negative mode.

\znl

Finally, we checked the different algorithms on a mix of sound signals, as ICA is often motivated by the "cocktail party problem".

%% --------- ---- -  -               figure               -  - ---- --------- %%


\begin{figure}
\centering
\includegraphics[width=.74\textwidth]{../image_test/unmix4.png}
\caption{Application of JADE algorithm to images separation. The first line presents the original sources, the second one the mix and the last one the estimations.\label{fig:res_images}}
\end{figure}

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                               FIN DU DOCUMENT                                %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %%

\bibliographystyle{alpha}
\bibliography{Biblio}{}
%\nocite{*}

\label{lastpage}

\end{document}

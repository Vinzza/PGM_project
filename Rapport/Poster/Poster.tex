\documentclass{beamer}
\mode<presentation>
{\usetheme{Berlin}}
\usepackage[orientation=portrait,size=a1,scale=1.0]{beamerposter}

\input{zMC.tex}
\input{zMCgraphe.tex}

\zzpackages[english]


%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                MACRO LOCALES                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %%

\newcommand{\mysection}[1]{\vspace{-0pt}\section{#1}\vspace{0pt}}
\newcommand{\mysubsection}[1]{\vspace{-7pt}\subsection{\normalsize #1}\vspace{-2pt}}


\newcommand{\Kl}[3][]{\mathrm K_{#1}\!\zp{#2\:\|\:#3}}
\newcommand{\zZ}[2]{\mathrm #1\!\zp{#2}}
\newcommand{\zD}{\mathcal}
\newcommand{\Ng}[2]{\mathcal{N}\zp{#1,\:#2}}

\usefonttheme{professionalfonts} % using non standard fonts for beamer
\usefonttheme{serif} % default family is serif
%\usepackage{fontspec}
%\setmainfont{Liberation Serif}

\tikzset{
  zplot/.style={opacity=.8}
}

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                    TITRE                                     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\graphicspath{{ figures/}}
\title{\Huge Independant Componant Analysis}
\author[Lara, Tilquin, Vidal]
{
	\parbox{.25\textwidth}{\includegraphics[height=4cm]{ENS_cachan.pdf}}%
	\parbox{.5\textwidth}{\hfil \huge Nathan de Lara, Florian Tilquin et Vincent Vidal \hfil}%
	\parbox{.25\textwidth}{\hspace{2cm} \includegraphics[height=4cm]{UPS.png}}%
}

\institute[Université Paris-Saclay]{\LARGE Master Mathématiques, Vision et Apprentissage}
\date{}

\usebackgroundtemplate%
{%
	    \includegraphics[width=\paperwidth,height=\paperheight]{watermark.jpg}%
	}
	\addtobeamertemplate{block begin}{\pgfsetfillopacity{0.8}}{\pgfsetfillopacity{1}}

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                DEBUT DOCUMENT                                %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %%
        
\begin{document}
\begin{frame}{}
	\maketitle
\begin{columns}[T]
\begin{column}{.48\linewidth}

%% --------- ---- -  -        SEPARATION DE BLOCS         -  - ---- --------- %%

\begin{block}{Problem statement}
Let $x\in\zR^p$ be some random variables whose components correspond to different mix of some primitive sources $s_i\in\zR^n$. The aim is to retrieve an estimation $y$ of every source $s_i$, given only $x$. We note $A$ the mixing matrix and $W$ the separation matrix such that:
\begin{equation}
\label{eqn:pb}
 x = A s \ \ \mbox{and} \ \ y = W x.
\end{equation}

In order to retrieve the sources, we suppose that:\begin{itemize}
\item \ the sources are independents
\item \ the mix is linear and instantaneous
\item \ at most one source has a Gaussian distribution.  
\end{itemize}
\end{block}

%% --------- ---- -  -        SEPARATION DE BLOCS         -  - ---- --------- %%

\begin{block}{Measure of independance}
We want to find $W$ that maximises the independence of $y=Wx$.

The information theory gives us quantity to work with, based on the Kullback-Leibler divergence:
\begin{equation}
  \Kl PQ = \int_{\zR^n} P(x) \log\frac{P(x)}{Q(x)} \zdx x.
\end{equation}
With $\zD G$ the gaussian distribution manifold and $\zD P$ the product one, we define

\begin{figure}
\label{fig:sketch}
\centering
  \includegraphics[height = 11cm, width=\textwidth]{../figure_tikz/theory_info}
  \caption{Representation of the distribution and the different projections on the manifolds $\zD P$ and $\zD G$ and the different quantities that can be define with the Kullback Leibler divergence.}
\end{figure}

The Pythagorean theorem gives the relation
\begin{equation}
\label{eqn:pyt}
        \zZ IY + \sum_i\zZ G{Y_i} = \zZ GY + \zZ CY.
\end{equation}
If the mutual information $\zZ I P$ appears to be the best one to use, it is to hard to compute.
The equation \ref{eqn:pyt} justifies the use of the non-gaussianity, correlation or even negentropy as contrast functions.

\end{block}

%% --------- ---- -  -        SEPARATION DE BLOCS         -  - ---- --------- %%

\begin{block}{Performance evaluation}
We use the ``Amari distance'', equation \ref{eqn:amari}, which gives a criterion of proximity between two matrices, to evaluate the performance of an algorithm.

If $U$ and $V$ are two $n$-by-$n$ matrices, the Amari distance is defined by:
\begin{equation} \label{eqn:amari}
	d(U,V) = \frac{1}{2n}\sum\limits_{i=1}^n \left(\frac{\sum\limits_{j=1}^n|a_{ij}|}{\max_j |a_{ij}|}-1 \right)+\frac{1}{2n}\sum\limits_{j=1}^n \left(\frac{\sum\limits_{i=1}^n|a_{ij}|}{\max_i |a_{ij}|}-1 \right)
\end{equation}
with $a_{ij} = (UV^{-1})_{ij}$.
This function has the advantage to be invariant by scaling factors and permutation of the components of the matrices.

\end{block}

%% --------- ---- -  -        SEPARATION DE BLOCS         -  - ---- --------- %%

\begin{block}{Algorithms}
Several algorithms have been developed to perform ICA, among those we can cite:
\hbox to \textwidth{\hss\textbullet\ HJ \hss\textbullet\ JADE \hss\textbullet\ FastICA \hss\textbullet\ KernelICA \hss }

%One way to compare these algorithms is by their estimated mixing (ou unmixig) matrices : given numerous generated i.i.d. sources and a mixing matrix, all of these algorithms will return the best estimated mixing matrix according to their corresponding method.
%Thus we could compare the estimations with our original mixing matrix with any norm whatsoever, but as we previously said, the algorithms are not able to find out the original order of the sources or their amplitude.
\end{block}

%% --------- ---- -  -        SEPARATION DE BLOCS         -  - ---- --------- %%


\end{column}

% Deuxième colonne
\begin{column}{.48\linewidth}

\begin{block}{Hérault and Jutten (HJ) algorithm}
This method is based on the neural network principle.
We write $W = \zp{I_n+\widetilde W}^{-1}$ and for a pair of given functions $\zp{f,\:g}$, we adapt the $\widetilde W$ as follows:\begin{equation}
\widetilde W_{ij} = f(y_i) g(y_j).
\end{equation}
\end{block}

%% --------- ---- -  -        SEPARATION DE BLOCS         -  - ---- --------- %%

\begin{block}{Jade algorithm}
Several methods are based on the cumulants. The aim here is to annul all the cross cumulants of order $4$.
Thus, we diagonalize the cumulant tensor which is equivalent to minimise the following contrast function:
\begin{equation}
  c\zp{x} = \sum_{i,k,l}\zp[b]{\zop{Cum}{x_i,x_i^*,x_k,x_l}}^2.
\end{equation}
\end{block}

%% --------- ---- -  -        SEPARATION DE BLOCS         -  - ---- --------- %%

\begin{block}{FastICA algorithm}
The FastICA algorithm is based on the information theory. We want here to maximise the marginal non-gaussianity on the whitened data, relying on a non linear quadratic function $f$ with the following rule:\begin{equation}
  \widetilde W_{t+1} = \zesp{X.\ztr{f(\ztr{W_t}X)}} - \zesp{f''(\ztr{W_t}X)}W_t,
\end{equation}
with $W_t$ the normalise vector of $\widetilde W_t$. In our case, we will use $f(x) = \frac{x^4}4$. We may use $f(x) = \log \cosh x$ or $f(x) = \exp\zp{-\frac{x^2}2}$ too.
\end{block}

%% --------- ---- -  -        SEPARATION DE BLOCS         -  - ---- --------- %%

\begin{block}{Kernel ICA}
\vspace{3cm}... À faire ...\vspace{3cm}
\end{block}

%% --------- ---- -  -        SEPARATION DE BLOCS         -  - ---- --------- %%

\begin{block}{Modus operandi}
We have consider $m$ distribution of probability, each sampled $N$ times.

We mixed them with a random matrix, whitened this data and apply the ICA algorithm.

Eventually, we compare the matrix found with the true matrix using the Amari distance.
\end{block}

%% --------- ---- -  -        SEPARATION DE BLOCS         -  - ---- --------- %%

\begin{block}{Results}
	\begin{figure}
	\includegraphics[width=\textwidth]{unmix_images}
\caption{Results of the main ICA algorithms on simulated data. Each colour corresponds to a signal. The ``Sources'' graph shows the unmixed signal    s and the other ones the results of the algorithms. \label{fig:res}}
\end{figure}
\end{block}

\end{column}
\end{columns}

\end{frame}
\end{document}

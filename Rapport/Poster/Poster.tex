\documentclass{beamer}
\mode<presentation>
{\usetheme{Rochester}}
\usepackage[orientation=portrait,size=a1,scale=0.9]{beamerposter}

\input{zMC.tex}
\input{zMCgraphe.tex}

\zzpackages[english]


%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                MACRO LOCALES                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %%

\newcommand{\mysection}[1]{\vspace{-0pt}\section{#1}\vspace{0pt}}
\newcommand{\mysubsection}[1]{\vspace{-7pt}\subsection{\normalsize #1}\vspace{-2pt}}


\newcommand{\Kl}[3][]{\mathrm K_{#1}\!\zp{#2\:\|\:#3}}
\newcommand{\zZ}[2]{\mathrm #1\!\zp{#2}}
\newcommand{\zD}{\mathcal}
\newcommand{\Ng}[2]{\mathcal{N}\zp{#1,\:#2}}

\usefonttheme{professionalfonts} % using non standard fonts for beamer
\usefonttheme{serif} % default family is serif
%\usepackage{fontspec}
%\setmainfont{Liberation Serif}

\tikzset{
  zplot/.style={opacity=.8}
}

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                    TITRE                                     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\graphicspath{{ figures/}}
\title{\Huge Independant Componant Analysis}
\author[Lara, Tilquin, Vidal]
{
	\parbox{.25\textwidth}{\includegraphics[height=4cm]{ENS_cachan.pdf}}%
	\parbox{.5\textwidth}{\hfil \large \textbf{Nathan de Lara, Florian Tilquin, Vincent Vidal} \hfil}%
	\parbox{.25\textwidth}{\hspace{2cm} \includegraphics[height=4cm]{UPS.png}}%
}

\institute[Université Paris-Saclay]{\huge \textbf{Master Mathématiques, Vision et Apprentissage}}
\date{}

\usebackgroundtemplate%
{%
	    \includegraphics[width=\paperwidth,height=\paperheight]{cloud.jpg}%
	}
	\addtobeamertemplate{block begin}{\pgfsetfillopacity{0.8}}{\pgfsetfillopacity{1}}

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                DEBUT DOCUMENT                                %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %%
        
\begin{document}
\begin{frame}{}
	\maketitle
\begin{columns}[T]
\begin{column}{.48\linewidth}

%% --------- ---- -  -        SEPARATION DE BLOCS         -  - ---- --------- %%

\begin{block}{Problem statement}
Let $x\in\zR^p$ be some random variables whose components correspond to different mix of some primitive sources $s_i\in\zR^n$. The aim is to retrieve an estimation $y$ of every source $s_i$, given only $x$. We note $A$ the mixing matrix and $W$ the separation matrix such that:
\begin{equation}
\label{eqn:pb}
 x = A s \ \ \mbox{and} \ \ y = W x.
\end{equation}

In order to retrieve the sources, we suppose that:\begin{itemize}
\item \ the sources are independents
\item \ the mix is linear and instantaneous
\item \ at most one source has a Gaussian distribution.  
\end{itemize}

Sometimes called \textit{blind source separation problem}, ICA is for example used for denoising or artifacts removal in financial or EEG data.
\end{block}

%% --------- ---- -  -        SEPARATION DE BLOCS         -  - ---- --------- %%

\begin{block}{Measure of independence}
The idea to solve the problem~\ref{eqn:pb} is to estimate $W$ such that the independence of $y$ is maximal.
The information theory provides a measure of independence based on the Kullback-Leibler divergence:
\begin{equation}
  \Kl PQ = \int_{\zR^n} P(x) \log\frac{P(x)}{Q(x)} \zdx x.
\end{equation}



\hbox to \textwidth{
\begin{minipage}{.46\textwidth}
With $\zD G$ the Gaussian distribution manifold, $\zD P$ the product one and $P^{\zD G}$ and $P^{\zD P}$ the projection of those distribution. We define:
\begin{itemize}

\item \ The Mutual Information
\begin{equation}
\zZ IY = \Kl {\zpbig2 P(Y)}{\Pi_i P_i(Y_i)}  
\end{equation}

\item \ The Non-Gaussianity
\begin{equation}
  \zZ GY = \Kl{Y}{\Ng{\zesp Y}{\Sigma_Y}\zpbig2}
\end{equation}

\item \ The Correlation
\begin{equation}
  \Kl{\zpbig2 P(Y)^{\zD G}}{P(Y)^{\zD P\wedge\zD G}}
\end{equation}
\end{itemize}

\end{minipage}
\hss
\begin{minipage}{.5\textwidth}
\begin{figure}
\label{fig:sketch}
\centering
  \includegraphics[height = 11cm, width=\textwidth]{../figure_tikz/theory_info}
  \caption{Representation of the distribution and the different projections on the manifolds $\zD P$ and $\zD G$.}
\end{figure}
\end{minipage}
}

As mentioned in~\cite{}, the Pythagorean theorem implies:
\begin{equation}
\label{eqn:pyt}
        \zZ IY + \sum_i\zZ G{Y_i} = \zZ GY + \zZ CY.
\end{equation}
If the mutual information $\zZ I P$ appears to be the best one to use, it is too hard to compute.
The equation \ref{eqn:pyt} justifies the use of the non-gaussianity, correlation or even negentropy as contrast functions.

\end{block}

%% --------- ---- -  -        SEPARATION DE BLOCS         -  - ---- --------- %%

\begin{block}{Performance evaluation}
The ``Amari error'', equation \ref{eqn:amari}, gives a criterion of proximity between two matrices, to evaluate the performance of an algorithm.
If $U$ and $V$ are two $n$-by-$n$ matrices, the Amari error is defined by:
\begin{equation} \label{eqn:amari}
	d(U,V) = \frac{1}{2n}\sum\limits_{i=1}^n \left(\frac{\sum\limits_{j=1}^n|a_{ij}|}{\max_j |a_{ij}|}-1 \right)+\frac{1}{2n}\sum\limits_{j=1}^n \left(\frac{\sum\limits_{i=1}^n|a_{ij}|}{\max_i |a_{ij}|}-1 \right)
\end{equation}
with $a_{ij} = (UV^{-1})_{ij}$.
This function, which is not an actual distance, has the advantage to be invariant by scaling factors and permutation of the components of the matrices.

\end{block}

%% --------- ---- -  -        SEPARATION DE BLOCS         -  - ---- --------- %%

\begin{block}{Algorithms}
Several algorithms have been developed to perform ICA, among those we can cite:
\begin{itemize}
\item HJ: one of the first algorithms for ICA by Hérault and Jutten who pioneered the \textit{blind source separation problem} in the 1980s. It is inspired from the neural network principle.
\item JADE: for Joint Approximate Diagonalization of Eigenmatrices. It belongs to the family of the cumulants algorithms initially introduced by Comon in the early 1990s and has a complexity of $\mathcal{O}(n^4)$.
\item FastICA: proposed by Hyvärinen in the late 1990s, it uses non-gaussianity as an approximation of independence and performs approximate Newton iterations.
\item KernelICA: Introduced in the early 2000s by Bach and Jordan, this method outperforms most of the previous algorithms and is particularly resistant to outliers. However, it is more computationally expensive.
\end{itemize}

\end{block}

%% --------- ---- -  -        SEPARATION DE BLOCS         -  - ---- --------- %%
\begin{block}{Hérault and Jutten (HJ) algorithm}
This method is based on the neural network principle. Writing $W = \zp{I_n+\widetilde W}^{-1}$, for a pair of given functions $\zp{f,\:g}$, the algorithm estimates:
\begin{equation}
\widetilde W_{ij} = f(y_i) g(y_j).
\end{equation}
\end{block}

\end{column}

% Deuxième colonne
\begin{column}{.48\linewidth}



%% --------- ---- -  -        SEPARATION DE BLOCS         -  - ---- --------- %%

\begin{block}{JADE algorithm}
Several methods are based on the cumulants. The aim in JADE is to annul all the cross cumulants of order $4$.
The cumulant tensor is diagonalized which is equivalent to minimizing the following contrast function:
\begin{equation}
  c\zp{x} = \sum_{i,k,l}\zp[b]{\zop{Cum}{x_i,x_i^*,x_k,x_l}}^2.
\end{equation}
\end{block}

%% --------- ---- -  -        SEPARATION DE BLOCS         -  - ---- --------- %%

\begin{block}{FastICA algorithm}
The FastICA algorithm is based on the information theory. Non-gaussianity is used a proxy for independence. Given a quadratic function $f$, the algorithm performs:
\begin{equation}
  \widetilde W_{t+1} = \zesp{X.\ztr{f(\ztr{W_t}X)}} - \zesp{f''(\ztr{W_t}X)}W_t,
\end{equation}
where $W_t$ is the normalized vector of $\widetilde W_t$. For experimentation, we use $f(x) = \frac{x^4}4$.
\end{block}

%% --------- ---- -  -        SEPARATION DE BLOCS         -  - ---- --------- %%

\begin{block}{KernelICA algorithm}
Given a reproducing kernel Hilbert space $\mathcal{F}$, this algorithm seeks to minimize the Kernel Generalized Variance defined as:
\begin{equation}
	\widehat{\delta}_{\mathcal{F}}=-\frac{1}{2}\log \underset{i}{\prod}(1-\rho_i^2)
\end{equation}
where the $\rho_i$ are the kernel canonical correlations between the observations components, obtained with computations over the observations Gram matrices.
\end{block}

%% --------- ---- -  -        SEPARATION DE BLOCS         -  - ---- --------- %%

%\begin{block}{Modus operandi}
%We have consider $m$ distribution of probability, each sampled $N$ times.
%
%We mixed them with a random matrix, whitened this data and apply the ICA algorithm.
%
%Eventually, we compare the matrix found with the true matrix using the Amari error.
%\end{block}

%% --------- ---- -  -        SEPARATION DE BLOCS         -  - ---- --------- %%

\begin{block}{Results}

\begin{figure}
\label{distres}
\centering
\resizebox{\textwidth}{!}{
\input{Tableau_Nathan.txt}
\input{Tableau2.txt}}
\caption{\textbf{Left:} Average Amari error re-scaled by 100 obtained with the listed algorithms for random mix $m=2$ sources of size $N=250$ sampled with twelve different distributions. \textbf{Right:} Same measure for $m$ sources of size $N$ whose distribution is randomly selected among the twelve. The best results are in bold font. An X is put when a standard desktop computer could not compute the result.}
\end{figure}

\begin{figure}
\label{imres}
\centering
\includegraphics[width=16cm]{../../image_test/unmix4.png}
\caption{Application of JADE algorithm to images separation. The first line presents the original sources, the second one the mix and the last one the estimations.}
\end{figure}
\end{block}

\begin{block}{References}
	\begin{tiny}
\bibliographystyle{alpha}
\bibliography{Biblio}{}
\nocite{*}
	\end{tiny}
\end{block}

\end{column}
\end{columns}

\end{frame}
\end{document}
